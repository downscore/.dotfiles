#!/usr/bin/env -S uv run --script
# /// script
# dependencies = ["trafilatura", "beautifulsoup4", "curl_cffi", "playwright", "playwright-stealth"]
# ///
"""Fetch a URL and extract its main text content, or download PDFs.

For HTML pages, extracts text via trafilatura (with BeautifulSoup fallback) and
prints to stdout. Output is auto-truncated to ~28k chars to fit in Claude Code's
Bash tool output limit. Use -o to save to a specific path (skips stdout, prints
path only).

For PDF URLs (detected by Content-Type or .pdf extension), downloads the file and
saves locally. Always use -o with a descriptive filename for PDFs. If -o is not
given, saves to a temp file. Prints the saved path. Use the Read tool to view.

Uses curl_cffi to impersonate Chrome's TLS fingerprint, bypassing bot detection
on sites that reject Python's default TLS handshake. For sites with JavaScript
challenges (e.g. Akamai Bot Manager), use -b to render with headless Chromium.

Requires one-time setup for -b: playwright install chromium

Exit codes:
    0 - Success
    1 - Network or HTTP error
    2 - Empty response from server
    3 - Content extraction failed (page fetched but no usable text found)
"""

import argparse
import hashlib
import os
import sys
import tempfile
import urllib.parse

from curl_cffi import requests as cffi_requests
import trafilatura
from bs4 import BeautifulSoup

# Claude Code's Bash tool truncates at 30k chars; leave headroom for warnings/framing.
OUTPUT_CHAR_LIMIT = 28_000


def _error(msg, code, quiet=False):
    if not quiet:
        print(msg, file=sys.stderr)
    sys.exit(code)


def _bs4_fallback(html):
    """Fallback: use BeautifulSoup to extract text content."""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "nav", "header", "footer"]):
        tag.decompose()
    text = soup.get_text(separator="\n", strip=True)
    return text if len(text) > 100 else None


def _is_pdf(url, content_type):
    """Check if the response is a PDF by Content-Type or URL extension."""
    if content_type and "application/pdf" in content_type:
        return True
    return urllib.parse.urlparse(url).path.lower().endswith(".pdf")


def _extract_text(html, url, quiet=False):
    """Extract text content from HTML via trafilatura with bs4 fallback."""
    text = trafilatura.extract(
        html,
        include_comments=False,
        include_tables=True,
        favor_recall=True,
    )

    if not text:
        if not quiet:
            print("[trafilatura failed, falling back to BeautifulSoup]", file=sys.stderr)
        text = _bs4_fallback(html)

    if not text:
        _error(f"No content extracted from {url}", 3, quiet)

    return {"type": "text", "text": text}


def _save_file(content, url, output_path=None, ext="txt", binary=False):
    """Save content to the given path, or a deterministic temp file based on URL hash."""
    if output_path:
        path = output_path
    else:
        url_hash = hashlib.md5(url.encode()).hexdigest()[:10]
        path = os.path.join(tempfile.gettempdir(), f"webfetch-{url_hash}.{ext}")
    with open(path, "wb" if binary else "w") as f:
        f.write(content)
    return path


def fetch(url, quiet=False):
    """Fetch a URL. Returns {"type": "pdf", "data": bytes} or {"type": "text", "text": str}."""
    try:
        resp = cffi_requests.get(url, impersonate="chrome", timeout=15)
        resp.raise_for_status()
    except Exception as e:
        _error(f"Failed to fetch {url}: {e}", 1, quiet)

    content_type = resp.headers.get("Content-Type", "")

    if _is_pdf(url, content_type):
        if not resp.content:
            _error(f"Empty response from {url}", 2, quiet)
        return {"type": "pdf", "data": resp.content}

    downloaded = resp.text
    if not downloaded:
        _error(f"Empty response from {url}", 2, quiet)

    return _extract_text(downloaded, url, quiet)


def fetch_browser(url, quiet=False):
    """Fetch a URL using headless Chromium. Handles JS challenges."""
    from playwright.sync_api import sync_playwright
    from playwright_stealth import Stealth

    try:
        with Stealth().use_sync(sync_playwright()) as p:
            # If this fails, run: playwright install chromium
            browser = p.chromium.launch()
            page = browser.new_page()

            # Load page and give JS time to render. networkidle is unreliable
            # (SPAs never idle; challenge pages reload mid-wait), so we use a
            # fixed wait after load, which covers both cases.
            page.goto(url, wait_until="load", timeout=30000)
            page.wait_for_timeout(3000)

            content_type = page.evaluate(
                "() => document.contentType || ''"
            )

            if _is_pdf(url, content_type):
                # PDF downloads don't work well in headless mode.
                browser.close()
                return fetch(url, quiet)

            html = page.content()
            browser.close()
    except Exception as e:
        _error(f"Failed to fetch {url}: {e}", 1, quiet)

    if not html:
        _error(f"Empty response from {url}", 2, quiet)

    return _extract_text(html, url, quiet)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("url", help="URL to fetch")
    parser.add_argument("-b", "--browser", action="store_true", help="use headless Chromium (for JS-rendered pages)")
    parser.add_argument("-q", "--quiet", action="store_true", help="suppress error messages on stderr")
    parser.add_argument("-o", "--output", help="save to this path, print path only")
    args = parser.parse_args()

    if args.browser:
        result = fetch_browser(args.url, quiet=args.quiet)
    else:
        result = fetch(args.url, quiet=args.quiet)

    if result["type"] == "pdf":
        path = _save_file(result["data"], args.url, args.output, ext="pdf", binary=True)
        if not args.quiet:
            print(f"[PDF saved to: {path} — use Read tool to view]", file=sys.stderr)
        print(path)
    elif args.output:
        path = _save_file(result["text"], args.url, args.output)
        print(path)
    elif len(result["text"]) > OUTPUT_CHAR_LIMIT:
        path = _save_file(result["text"], args.url)
        truncated = result["text"][:OUTPUT_CHAR_LIMIT]
        # Try to cut at a newline to avoid mid-sentence truncation.
        last_nl = truncated.rfind("\n")
        if last_nl > OUTPUT_CHAR_LIMIT * 0.8:
            truncated = truncated[:last_nl]
        print(truncated)
        total = len(result["text"])
        shown = len(truncated)
        pct = shown * 100 // total
        print(f"\n[TRUNCATED: {pct}% shown ({shown:,}/{total:,} chars). Full text saved to: {path} — read it, don't re-fetch.]", file=sys.stderr)
    else:
        print(result["text"])
